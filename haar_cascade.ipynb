{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/Complete_ComputerVisiom_Krishn/blob/main/haar_cascade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yiCqFCHqQgk"
      },
      "outputs": [],
      "source": [
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z129c71BqQgl"
      },
      "outputs": [],
      "source": [
        "# Face Detection\n",
        "\n",
        "cam = cv2.VideoCapture(0)\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "while True:\n",
        "    ret, frame = cam.read()\n",
        "\n",
        "    if not ret:\n",
        "        print(\"Failed to grab frame\")\n",
        "        break\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "\n",
        "        cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "        cv2.putText(frame, \"Face Detected\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2, cv2.LINE_AA)\n",
        "\n",
        "    cv2.imshow('Camera', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cam.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Haar-cascade based face detection\n",
        "This code uses the OpenCV (cv2) library in Python to perform real-time face detection using your computer's camera.\n",
        "\n",
        "----\n",
        "----\n",
        "#1. Initialization:\n",
        "\n",
        "##cam = cv2.VideoCapture(0):\n",
        "This line initializes the camera.\n",
        "\n",
        "0 usually refers to the default webcam.\n",
        "\n",
        "##face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'):\n",
        "\n",
        "This line loads a pre-trained Haar Cascade classifier for frontal face detection.\n",
        "\n",
        "Haar Cascades are a machine learning-based approach for object detection.\n",
        "\n",
        "--------------\n",
        "#2. Main Loop:\n",
        "\n",
        "##while True::\n",
        "This starts an infinite loop that runs continuously until you press the 'q' key.\n",
        "\n",
        "##ret, frame = cam.read():\n",
        "This line reads a frame from the camera. ret is a boolean indicating if the frame was read successfully, and frame is the actual image frame.\n",
        "\n",
        "##if not ret: ... break:\n",
        "If the frame wasn't read successfully, it prints an error message and breaks the loop.\n",
        "\n",
        "##gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY):\n",
        "This line converts the frame to grayscale. Face detection often works better on grayscale images.\n",
        "\n",
        "-------------\n",
        "#3. Face Detection:\n",
        "\n",
        "##faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5):\n",
        "This is the core face detection step.\n",
        "\n",
        "It uses the loaded classifier to detect faces in the grayscale image.\n",
        "\n",
        "##face_cascade:\n",
        "This is an object of the cv2.\n",
        "\n",
        "CascadeClassifier class.\n",
        "\n",
        "It holds the pre-trained Haar Cascade classifier that you loaded earlier.\n",
        "\n",
        "This classifier has been trained to recognize patterns associated with frontal faces.\n",
        "\n",
        "##detectMultiScale:\n",
        "This is a method of the cv2.\n",
        "\n",
        "##CascadeClassifier class.\n",
        "It's the function that actually performs the face detection.\n",
        "\n",
        "It takes the image as input and tries to find regions within the image that match the patterns of a frontal face.\n",
        "\n",
        "##How it works:\n",
        "The detectMultiScale function uses a sliding window approach.\n",
        "\n",
        "It essentially moves a window of varying sizes across the image, looking for areas that resemble a face according to the Haar Cascade classifier.\n",
        "\n",
        "If it finds a region that scores high enough (meaning it closely matches the face pattern), it considers that region as a potential face.\n",
        "\n",
        "##Output:\n",
        "The detectMultiScale function returns a list of rectangles.\n",
        "\n",
        "Each rectangle represents a detected face in the image.\n",
        "\n",
        "The rectangle is defined by four values: (x, y, w, h), where (x, y) represents the top-left corner of the rectangle, and w and h represent the width and height of the rectangle, respectively.\n",
        "\n",
        "##Parameters:\n",
        "The detectMultiScale function takes several optional parameters that you can use to fine-tune the detection process.\n",
        "\n",
        "These include:\n",
        "\n",
        "##scaleFactor:\n",
        "Controls how much the image size is reduced at each image scale.\n",
        "\n",
        "A smaller scale factor increases the detection accuracy but also increases the processing time.\n",
        "\n",
        "##minNeighbors:\n",
        "Specifies how many neighbors each candidate rectangle should have to retain it.\n",
        "\n",
        "A higher value reduces the number of false positives but might miss some faces.\n",
        "\n",
        "##minSize:\n",
        "Specifies the minimum possible object size. Objects smaller than that are ignored.\n",
        "\n",
        "##maxSize:\n",
        "Specifies the maximum possible object size. Objects larger than that are ignored.\n",
        "\n",
        "##In summary,\n",
        "face_cascade.detectMultiScale is the heart of the face detection process in this code.\n",
        "\n",
        "It takes the image and the pre-trained classifier as input, scans the image for face-like regions, and returns a list of rectangles representing the detected faces.\n",
        "\n",
        "##scaleFactor:\n",
        "This parameter controls how the image size is reduced at each image scale.\n",
        "\n",
        "A smaller scale factor increases the detection accuracy but also increases the processing time.\n",
        "\n",
        "##minNeighbors:\n",
        "This parameter specifies how many neighbors each candidate rectangle should have to retain it.\n",
        "\n",
        "A higher value reduces the number of false positives but might miss some faces.\n",
        "\n",
        "----------------\n",
        "#4. Drawing Rectangles and Labels:\n",
        "\n",
        "##for (x,y,w,h) in faces::\n",
        "This loop iterates through all the detected faces.\n",
        "\n",
        "##cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 0, 0), 2):\n",
        "This line draws a blue rectangle around the detected face on the original color frame.\n",
        "\n",
        "##cv2.putText(frame, \"Face Detected\", (x, y - 10),\n",
        "\n",
        "This line uses the cv2.putText function from the OpenCV library to add text to an image.\n",
        "\n",
        " Here's a breakdown of the arguments:\n",
        "\n",
        "##frame:\n",
        "This is the image (or frame in this case) where you want to add the text.\n",
        "\n",
        "It's the same frame variable that's being captured from the camera in your code.\n",
        "\n",
        "##\"Face Detected\":\n",
        "This is the actual text string that you want to display on the image.\n",
        "\n",
        "##(x, y - 10):\n",
        "This is a tuple representing the coordinates where the text will be placed.\n",
        "\n",
        "x and y are the coordinates of the top-left corner of the detected face (obtained from face_cascade.detectMultiScale).\n",
        "\n",
        "y - 10 positions the text 10 pixels above the top of the detected face's bounding box.\n",
        "\n",
        "The remaining arguments in the cv2.putText function (which are not included in your snippet) control the font, size, color, thickness, and other aspects of the text.\n",
        "\n",
        "In simpler terms, this line of code adds the text \"Face Detected\" above the detected face in the video frame.\n",
        "\n",
        "It uses the coordinates of the detected face to position the text appropriately.\n",
        "\n",
        "\n",
        "#cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2, cv2.LINE_AA):\n",
        "\n",
        "This line adds the text \"Face Detected\" above the rectangle.\n",
        "\n",
        "These arguments further customize the appearance of the text that is added to the image:\n",
        "\n",
        "##cv2.FONT_HERSHEY_SIMPLEX:\n",
        "This specifies the font type for the text.\n",
        "\n",
        "In this case, it's using the Hershey Simplex font, which is a built-in sans-serif font in OpenCV.\n",
        "\n",
        "##0.9:\n",
        "This argument determines the font scale or size of the text.\n",
        "\n",
        "It's a floating-point value that scales the default font size. Here, it's set to 0.9, meaning the text will be slightly smaller than the default size.\n",
        "\n",
        "##(0, 255, 0):\n",
        "This is a tuple representing the color of the text in BGR (Blue, Green, Red) format.\n",
        "\n",
        "Here, it's set to (0, 255, 0), which corresponds to green color.\n",
        "\n",
        "##2:\n",
        "This argument specifies the thickness of the lines used to draw the text. It's an integer value, and in this case, it's set to 2, making the text lines relatively thick.\n",
        "\n",
        "##cv2.LINE_AA:\n",
        "This argument defines the line type used for drawing the text. cv2.LINE_AA stands for anti-aliased line, which produces smoother and less jagged text edges.\n",
        "\n",
        "##In summary,\n",
        "\n",
        "these arguments control the font type, size, color, thickness, and line type of the text \"Face Detected\" that is added to the image using cv2.putText.\n",
        "\n",
        "They allow you to customize the appearance of the text to make it more visible and readable within the video frame.\n",
        "\n",
        "I hope this clarifies the purpose of these arguments in the cv2.putText function\n",
        "\n",
        "----------------------\n",
        "#5. Displaying the Output:\n",
        "\n",
        "##cv2.imshow('Camera', frame):\n",
        "This line displays the frame with the detected faces in a window titled \"Camera\".\n",
        "\n",
        "##if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "\n",
        "##break:\n",
        "This line waits for a key press for 1 millisecond.\n",
        "\n",
        "If the 'q' key is pressed, it breaks the loop.\n",
        "\n",
        "---------------------\n",
        "#6. Cleanup:\n",
        "\n",
        "##cam.release():\n",
        "This line releases the camera.\n",
        "\n",
        "##cv2.destroyAllWindows():\n",
        "This line closes all OpenCV windows.\n",
        "\n",
        "----------------------------\n",
        "#In essence,\n",
        "this code continuously captures frames from your camera, detects faces in each frame, and draws rectangles around the detected faces, updating the display in real-time."
      ],
      "metadata": {
        "id": "4zj3qYu_q_OD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5_E32BLqQgm"
      },
      "outputs": [],
      "source": [
        "# face + eye + mouth\n",
        "\n",
        "cam = cv2.VideoCapture(0)\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "mouth_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')\n",
        "\n",
        "while True:\n",
        "    ret, frame = cam.read()\n",
        "\n",
        "    if not ret:\n",
        "        print(\"Failed to grab frame\")\n",
        "        break\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "\n",
        "        cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, \"Face Detected\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2, cv2.LINE_AA)\n",
        "\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "        roi_color = frame[y:y+h, x:x+w]\n",
        "\n",
        "        eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.3, minNeighbors=5)\n",
        "        mouths = mouth_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)\n",
        "\n",
        "        for (ex,ey, ew, eh) in eyes:\n",
        "            cv2.rectangle(frame, (x+ex,y+ey), (x+ex+ew, y+ey+eh), (255, 0, 0), 2)\n",
        "            cv2.putText(frame, \"Eye\", (x+ex,(y+ey) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2, cv2.LINE_AA)\n",
        "\n",
        "        for (mx,my, mw, mh) in mouths:\n",
        "            cv2.rectangle(frame, (x+mx,y+my), (x+mx+mw, y+my+mh), (255, 0, 0), 2)\n",
        "            cv2.putText(frame, \"Mouth\", (x+mx,(y+my) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2, cv2.LINE_AA)\n",
        "\n",
        "\n",
        "    cv2.imshow('Camera', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cam.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Detects faces, eyes, and mouths in real-time\n",
        "\n",
        "-------------\n",
        "----------\n",
        "#1. Initialization:\n",
        "\n",
        "Similar to before, the code initializes the camera using cv2.VideoCapture(0).\n",
        "\n",
        "It loads three Haar Cascade classifiers:\n",
        "\n",
        "face_cascade: for face detection.\n",
        "\n",
        "eye_cascade: for eye detection.\n",
        "\n",
        "mouth_cascade: for mouth detection.\n",
        "\n",
        "-----------------\n",
        "#2. Main Loop and Face Detection:\n",
        "\n",
        "The code enters a while True loop, continuously reading frames from the camera.\n",
        "\n",
        "It converts each frame to grayscale using cv2.cvtColor.\n",
        "\n",
        "It detects faces in the grayscale image using face_cascade.detectMultiScale.\n",
        "\n",
        "-------------------------\n",
        "#3. Region of Interest (ROI) and Eye/Mouth Detection:\n",
        "\n",
        "For each detected face, it defines a region of interest (ROI) within the frame using array slicing:\n",
        "\n",
        "    roi_gray: grayscale ROI containing the face.\n",
        "    roi_color: color ROI containing the face.\n",
        "\n",
        "It then uses eye_cascade.detectMultiScale and mouth_cascade.detectMultiScale to detect eyes and mouths within the ROI, respectively.\n",
        "\n",
        "##ROI stands for Region of Interest.\n",
        "\n",
        "In computer vision and image processing, ROI is a portion of an image that is selected for further processing or analysis.\n",
        "\n",
        "By focusing on a specific region, you can reduce the computational load and improve the efficiency of your algorithms.\n",
        "\n",
        "In your face detection code, ROI is used to narrow down the search area for eyes and mouths. After detecting a face, a rectangular region around the face is defined as the ROI.\n",
        "\n",
        "This region is then used to detect eyes and mouths within the face, rather than searching the entire image.\n",
        "\n",
        "##Here's how ROI is used in your code:\n",
        "\n",
        "    roi_gray = gray[y:y+h, x:x+w]\n",
        "    roi_color = frame[y:y+h, x:x+w]\n",
        "\n",
        "##roi_gray:\n",
        "This line extracts a grayscale ROI from the original grayscale image (gray).\n",
        "\n",
        "It uses array slicing to select the portion of the image defined by the coordinates and dimensions of the detected face (x, y, w, h).\n",
        "\n",
        "##roi_color:\n",
        "Similarly, this line extracts a color ROI from the original color image (frame) using the same coordinates and dimensions.\n",
        "\n",
        "\n",
        "By defining these ROIs, the subsequent eye and mouth detection steps (eye_cascade.detectMultiScale and mouth_cascade.detectMultiScale) are only applied to the relevant areas within the face, improving performance and accuracy.\n",
        "\n",
        "In summary, ROI in your code represents a rectangular region around the detected face, which is used to focus the search for eyes and mouths, making the detection process more efficient and targeted.\n",
        "\n",
        "------------------------\n",
        "#4. Drawing Rectangles and Labels:\n",
        "\n",
        "It draws rectangles and labels for the detected faces, eyes, and mouths using cv2.rectangle and cv2.putText.\n",
        "\n",
        "Notice that for eyes and mouths, the coordinates are adjusted relative to the face's position (x+ex, y+ey, etc.).\n",
        "\n",
        "##Function:\n",
        "\n",
        "    cv2.rectangle\n",
        "    \n",
        "is used to draw a rectangle on an image.\n",
        "\n",
        "##Arguments:\n",
        "\n",
        "##frame:\n",
        "The image to draw on (the video frame in this case).\n",
        "\n",
        "##(x, y):\n",
        "Top-left corner coordinates of the rectangle (obtained from face detection).\n",
        "\n",
        "##(x+w, y+h):\n",
        "Bottom-right corner coordinates, calculated using the width (w) and height (h) of the detected face.\n",
        "\n",
        "##(255, 0, 0):\n",
        "Color of the rectangle (blue in BGR format).\n",
        "\n",
        "##2:\n",
        "Thickness of the rectangle lines.\n",
        "\n",
        "##Purpose:\n",
        "This line draws a blue rectangle around the detected face in the video frame, using the face's location and dimensions to determine the rectangle's position and size.\n",
        "\n",
        "\n",
        "---------------------\n",
        "#5. Display and Cleanup:\n",
        "\n",
        "The code displays the frame with the detected features using cv2.imshow.\n",
        "\n",
        "It breaks the loop when the 'q' key is pressed.\n",
        "\n",
        "Finally, it releases the camera and closes all windows using cam.release() and cv2.destroyAllWindows().\n",
        "\n",
        "--------------------\n",
        "#In essence,\n",
        "this code detects faces, eyes, and mouths in real-time video using Haar Cascade classifiers.\n",
        "\n",
        "By defining ROIs around the detected faces, it focuses the eye and mouth detection on relevant areas, improving efficiency and accuracy."
      ],
      "metadata": {
        "id": "u6VEu3_u22UM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms8scNMwqQgn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "KNCVU",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}